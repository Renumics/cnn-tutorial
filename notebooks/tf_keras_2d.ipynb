{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with MNIST\n",
    "This tutorial will get you started with Convolutional Neural Networks in Tensorflow using the MNIST dataset.<br>\n",
    "We will design a simple CNN for classification and create a workflow that includes every step from loading the dataset to building, training and visualizing the network.<br>\n",
    "You can work through the notebook by running [$\\blacktriangleright\\!|\\!$ `Run`] each code or text cell one after another. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow matplotlib renumics-spotlight scipy umap-learn --use-deprecated=legacy-resolver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining imports\n",
    "We start by importing  `numpy` for scientific computing, `matplotlib` for plotting, and the machine learning framework `tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input, MaxPool2D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "Next we load the MNIST dataset.<br>\n",
    "MNIST is a database with over 60,000 images of handwritten digits. We will use the MNIST dataset to train a classifier on handwritten digits.\n",
    "![](imgs/mnist.png)\n",
    "*Some MNIST examples.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images, train_labels = train_images[:6000], train_labels[:6000]\n",
    "test_images, test_labels = test_images[:1000], test_labels[:1000]\n",
    "\n",
    "train_images = train_images.reshape(*train_images.shape, 1) / 255.0\n",
    "test_images = test_images.reshape(*test_images.shape, 1) / 255.0\n",
    "\n",
    "all_images = np.append(train_images, test_images, 0)\n",
    "all_labels = np.append(train_labels, test_labels, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load four sets of data from the MNIST database:\n",
    "- the training examples `x_train`\n",
    "- the training labels `y_train`\n",
    "- the test examples `x_test`\n",
    "- the test labels `y_test`\n",
    "\n",
    "We only use a subset of the whole dataset to reduce training time.\n",
    "\n",
    "<details style=\"border-radius: 2px;border:1px solid #55AAAA;background:#DDEEEE;\">\n",
    "<summary>Click for details on the data.</summary>\n",
    "    <tt>train_images</tt> has dimension <tt>6000x28x28</tt>, which means it contains 6000 training images of size <tt>28x28</tt>. Respectively, <tt>test_images</tt> contains 1000 test images of size <tt>28x28</tt>.\n",
    "    We reshape the images and add a dimension of size <tt>1</tt>, which is the number of channels (would be <tt>3</tt> for colored images).<br>\n",
    "    The pixels are represented as numbers between 0 and 255. We rescale them to a range between 0 and 1.<br>\n",
    "<tt>train_labels</tt> and <tt>test_labels</tt> contain the training and test labels, represented as decimal numbers between 0 and 9.\n",
    "</details>\n",
    "\n",
    "Let's view an image from the training set. You can change the `sample_idx` and re-run the following cell to view some other example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 7\n",
    "sample_img = train_images[sample_idx].squeeze()\n",
    "sample_label = train_labels[sample_idx]\n",
    "plt.title(\"Example {} with Label {}\".format(sample_idx, sample_label))\n",
    "plt.imshow(sample_img).set_cmap(\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "Now we get down to business: building the neural network.<br>\n",
    "A CNN typically consists of multiple convolutional layers followed by pooling and some fully-connected layers, also known as dense layers, at the end.\n",
    "![](imgs/cnn.png)\n",
    "*A simple CNN. [Source.](https://res.mdpi.com/entropy/entropy-19-00242/article_deploy/html/images/entropy-19-00242-g001.png)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input((28, 28, 1))\n",
    "x = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")(input_img)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\")(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "backbone = Flatten()(x)\n",
    "\n",
    "x = Dense(units=128, activation=\"relu\")(backbone)\n",
    "head = Dense(units=32, activation=\"relu\")(x)\n",
    "class_probs = Dense(units=10, activation=\"softmax\")(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We design our network to consist of two convolutional layers which are followed by pooling layers and two dense layers at the end. The last layer has 10 outputs for our 10 digits.\n",
    "<details style=\\\"border-radius: 2px;border:1px solid #55AAAA;background:#DDEEEE;\\\">\n",
    "<summary>Click for details on the layers.</summary>\n",
    "The <tt>Conv2D</tt> layer expects several arguments besides the input tensor. <tt>filters</tt> is the number of feature maps, <tt>kernel_size</tt> the size of the convolution kernel, <tt>padding='same'</tt> preserves the image dimensions and <tt>activation</tt> let's you define the activation function applied after the layer.<br>\n",
    "The <tt>MaxPool2D</tt> layer is initialized with <tt>pool_size=2</tt>, so <tt>2x2</tt> squares of the input are aggregated to one cell with the maximum value.\n",
    "</details>\n",
    "<br>\n",
    "Next we construct and compile our model.<br>\n",
    "We use the Adam optimizer and a categorical crossentropy loss, which compares the distribution of the predictions with the true distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_img, class_probs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now we can start training.<br>\n",
    "We iteratively fit the train images to their respective class labels and evaluate the performance of our model on the test data.\n",
    "In the code cell below, `epochs` is the number of training rounds and `batch_size` is the number of examples that we collectively feed into the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_images, train_labels, validation_data=(test_images, test_labels), epochs=5, batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the default parameters we achieve a test accuracy of approximately 97% after 5 epochs. That is already pretty good. We can improve to over 99% by using more train data and/or improving the network architecture.\n",
    "Feel free to try out different parameters to see how they effect the speed and accuracy of the training. You could increase or decrease the number of train examples, change some layer parameters or even add additional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Lastly, we want to look at some of the results. We plot the first 150 test images, labeled with the predicted classes. Wrong predictions are colored red.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)\n",
    "_, axs = plt.subplots(10, 15, figsize=(16, 16))\n",
    "axs = axs.flatten()\n",
    "for img, label, prediction, ax in zip(test_images.squeeze(), test_labels, predictions, axs):\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"{} ({:.2f})\".format(predicted_class, prediction[predicted_class]))\n",
    "    if label == predicted_class:\n",
    "        ax.imshow(img).set_cmap(\"Greys\")\n",
    "    else:\n",
    "        ax.imshow(img).set_cmap(\"Reds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great work, you managed to build and train your first simple <tt>2D</tt> model for images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative analysis of the model\n",
    "In order to interpret the model results, we compute several additional outputs:\n",
    "- We take the output of the last CNN-layer as a similarity measure (\"embedding\")\n",
    "- We compute the entropy of the softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from scipy import stats\n",
    "\n",
    "embedding_model = Model(input_img, head)\n",
    "embeddings = embedding_model.predict(all_images)\n",
    "predictions_softmax = model.predict(all_images)\n",
    "predictions = np.argmax(predictions_softmax, axis=1)\n",
    "entropies = stats.distributions.entropy(predictions_softmax, axis=1)\n",
    "\n",
    "# reduce embedding to two dimensions\n",
    "reducer = umap.UMAP()\n",
    "reduced_embedding = reducer.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all the results in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=[\"image\", \"label\", \"predictions\", \"emb_x\", \"emb_y\"])\n",
    "\n",
    "df[\"image\"] = [all_images[i, :, :, :] for i in range(all_images.shape[0])]\n",
    "df[\"label\"] = all_labels\n",
    "df[\"prediction\"] = predictions\n",
    "\n",
    "df[\"emb_x\"] = reduced_embedding[:, 0]\n",
    "df[\"emb_y\"] = reduced_embedding[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize with Spotlight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from renumics import spotlight\n",
    "\n",
    "spotlight.show(df, port=8889, no_ssl=True, host=\"0.0.0.0\", layout=\"debug_layout.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the viewer if it is still running on port 8889\n",
    "spotlight.close(8889)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Compare two different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"model2_prediction\"] = predictions\n",
    "\n",
    "df[\"model2_emb_x\"] = reduced_embedding[:, 0]\n",
    "df[\"model2_emb_y\"] = reduced_embedding[:, 1]\n",
    "\n",
    "df[\"model1_correct\"] = df[\"prediction\"] == df[\"label\"]\n",
    "df[\"model2_correct\"] = df[\"model2_prediction\"] == df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from renumics import spotlight\n",
    "\n",
    "port = 8889\n",
    "spotlight.show(df, port=port, no_ssl=True, host=\"0.0.0.0\", layout=\"compare_layout.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 742.844,
   "position": {
    "height": "40px",
    "left": "1532px",
    "right": "20px",
    "top": "102px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
